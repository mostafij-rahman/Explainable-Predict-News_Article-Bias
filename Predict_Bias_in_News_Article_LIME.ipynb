{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5519f62e",
   "metadata": {},
   "source": [
    "# LIME to Inspect Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f055be",
   "metadata": {},
   "source": [
    "This tutorial focuses on showing how to use Captum's implementation of Local Interpretable Model-agnostic Explanations (LIME) to understand neural models. The following content is divided into an image classification section to present our high-level interface `Lime` class and a text classification section for the more customizable low-level interface `LimeBase`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa78370",
   "metadata": {},
   "source": [
    "## 2. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d15fc",
   "metadata": {},
   "source": [
    "In this section, we will take use of a news subject classification example to demonstrate more customizable functions in Lime. We will train a simple embedding-bag classifier on AG_NEWS dataset and analyze its understanding of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c pytorch torchtext==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd71ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _torchtext: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0817e9b8d9f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAG_NEWS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torchenv\\lib\\site-packages\\torchtext\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torchenv\\lib\\site-packages\\torchtext\\vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m from torchtext._torchtext import (\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mVocab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mVocabPybind\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _torchtext: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf1f4c",
   "metadata": {},
   "source": [
    "### 2.1 Load the data and define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31ba7",
   "metadata": {},
   "source": [
    "`torchtext` has included the AG_NEWS dataset but since it is only split into train & test, we need to further cut a validation set from the original train split. Then we build the vocabulary of the frequent words based on our train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebec7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 86716\n",
      "Num of classes: 4\n"
     ]
    }
   ],
   "source": [
    "ag_ds = list(AG_NEWS(split='train'))\n",
    "\n",
    "ag_train, ag_val = ag_ds[:100000], ag_ds[100000:]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "word_counter = Counter()\n",
    "for (label, line) in ag_train:\n",
    "    word_counter.update(tokenizer(line))\n",
    "voc = Vocab(word_counter)\n",
    "\n",
    "print('Vocabulary size:', len(voc))\n",
    "\n",
    "num_class = len(set(label for label, _ in ag_train))\n",
    "print('Num of classes:', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a1ef3",
   "metadata": {},
   "source": [
    "The model we use is composed of an embedding-bag, which averages the word embeddings as the latent text representation, and a final linear layer, which maps the latent vector to the logits. Unconventially, `pytorch`'s embedding-bag does not assume the first dimension is batch. Instead, it requires a flattened vector of indices with an additional offset tensor to mark the starting position of each example. You can refer to its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e94fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, inputs, offsets):\n",
    "        embedded = self.embedding(inputs, offsets)\n",
    "        return self.linear(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f66dd",
   "metadata": {},
   "source": [
    "### 2.2 Training and Baseline Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6b9f8",
   "metadata": {},
   "source": [
    "In order to train our classifier, we need to define a collate function to batch the samples into the tensor fomat required by the embedding-bag and create the interable dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a564e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels = torch.tensor([label - 1 for label, _ in batch]) \n",
    "    text_list = [tokenizer(line) for _, line in batch]\n",
    "    \n",
    "    # flatten tokens across the whole batch\n",
    "    text = torch.tensor([voc[t] for tokens in text_list for t in tokens])\n",
    "    # the offset of each example\n",
    "    offsets = torch.tensor(\n",
    "        [0] + [len(tokens) for tokens in text_list][:-1]\n",
    "    ).cumsum(dim=0)\n",
    "\n",
    "    return labels, text, offsets\n",
    "\n",
    "train_loader = DataLoader(ag_train, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(ag_val, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb29b3",
   "metadata": {},
   "source": [
    "We will then train our embedding-bag model with the common cross-entropy loss and Adam optimizer. Due to the simplicity of this task, 5 epochs should be enough to give us a stable 90% validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2e2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "EMB_SIZE = 64\n",
    "CHECKPOINT = './embedding_bag_ag_news.pt'\n",
    "USE_PRETRAINED = True  # change to False if you want to retrain your own model\n",
    "\n",
    "def train_model(train_loader, val_loader):\n",
    "    model = EmbeddingBagModel(len(voc), EMB_SIZE, num_class)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):      \n",
    "        # training\n",
    "        model.train()\n",
    "        total_acc, total_count = 0, 0\n",
    "        \n",
    "        for idx, (label, text, offsets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            predited_label = model(text, offsets)\n",
    "            loss(predited_label, label).backward()\n",
    "            optimizer.step()\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print('epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(\n",
    "                    epoch, idx + 1, len(train_loader), total_acc / total_count\n",
    "                ))\n",
    "                total_acc, total_count = 0, 0       \n",
    "        \n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        total_acc, total_count = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for label, text, offsets in val_loader:\n",
    "                predited_label = model(text, offsets)\n",
    "                total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "                total_count += label.size(0)\n",
    "\n",
    "        print('-' * 59)\n",
    "        print('end of epoch {:3d} | valid accuracy {:8.3f} '.format(epoch, total_acc / total_count))\n",
    "        print('-' * 59)\n",
    "    \n",
    "    torch.save(model, CHECKPOINT)\n",
    "    return model\n",
    "        \n",
    "eb_model = torch.load(CHECKPOINT) if USE_PRETRAINED else train_model(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e435b",
   "metadata": {},
   "source": [
    "Now, let us take the following sports news and test how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17495ba0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingBag' object has no attribute 'padding_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-be8c02740f43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_offsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_offsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction probability:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b88f6d5e43e3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, offsets)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[0;32m    385\u001b[0m                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m                                \u001b[0mper_sample_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minclude_last_offset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                                self.padding_idx)\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1131\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EmbeddingBag' object has no attribute 'padding_idx'"
     ]
    }
   ],
   "source": [
    "test_label = 2  # {1: World, 2: Sports, 3: Business, 4: Sci/Tec}\n",
    "test_line = ('US Men Have Right Touch in Relay Duel Against Australia THENS, Aug. 17 '\n",
    "            '- So Michael Phelps is not going to match the seven gold medals won by Mark Spitz. '\n",
    "            'And it is too early to tell if he will match Aleksandr Dityatin, '\n",
    "            'the Soviet gymnast who won eight total medals in 1980.')\n",
    "\n",
    "test_labels, test_text, test_offsets = collate_batch([(test_label, test_line)])\n",
    "\n",
    "probs = F.softmax(eb_model(test_text, test_offsets), dim=1).squeeze(0)\n",
    "print('Prediction probability:', round(probs[test_labels[0]].item(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759eb05",
   "metadata": {},
   "source": [
    "Our embedding-bag does successfully identify the above news as sports with pretty high confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2b535",
   "metadata": {},
   "source": [
    "### 2.3 Inspect the model prediction with Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180b749",
   "metadata": {},
   "source": [
    "Finally, it is time to bring back Lime to inspect how the model makes the prediction. However, we will use the more customizable `LimeBase` class this time which is also the low-level implementation powering the `Lime` class we used before. The `Lime` class is opinionated when creating features from perturbed binary interpretable representations. It can only set the \"absense\" features to some baseline values while keeping other \"presense\" features. This is not what we want in this case. For text, the interpretable representation is a binary vector indicating if the word of each position is present or not. The corresponding text input should literally remove the absent words so our embedding-bag can calculate the average embeddings of the left words. Setting them to any baselines will pollute the calculation and moreover, our embedding-bag does not have common baseline tokens like `<padding>` at all. Therefore, we have to use `LimeBase` to customize the conversion logic through the `from_interp_rep_transform` argument.\n",
    "\n",
    "`LimeBase` is not opinionated at all so we have to define every piece manually. Let us talk about them in order:\n",
    "- `forward_func`, the forward function of the model. Notice we cannot pass our model directly since Captum always assumes the first dimension is batch while our embedding-bag requires flattened indices. So we will add the dummy dimension later when calling `attribute` and make a wrapper here to remove the dummy dimension before giving to our model.\n",
    "- `interpretable_model`, the surrogate model. This works the same as we demonstrated in the above image classification example. We also use sklearn linear lasso here.\n",
    "- `similarity_func`, the function calculating the weights for training samples. The most common distance used for texts is the cosine similarity in their latent embedding space. The text inputs are just sequences of token indices, so we have to leverage the trained embedding layer from the model to encode them to their latent vectors. Due to this extra encoding step, we cannot use the util `get_exp_kernel_similarity_function('cosine')` like in the image classification example, which directly calculate the cosine similarity of the given inputs.\n",
    "- `perturb_func`, the function to sample interpretable representations. We present another way to define this argument other than using generator as shown in the above image classification example. Here we directly define a function returning a randomized sample every call. It outputs a binary vector where each token is selected independently and uniformly at random.\n",
    "- `perturb_interpretable_space`, whether perturbed samples are in interpretable space. `LimeBase` also supports sampling in the original input space, but we do not need it in our case.\n",
    "- `from_interp_rep_transform`, the function transforming the perturbed interpretable samples back to the original input space. As explained above, this argument is the main reason for us to use `LimeBase`. We pick the subset of the present tokens from the original text input according to the interpretable representation.\n",
    "- `to_interp_rep_transform`, the opposite of `from_interp_rep_transform`. It is needed only when `perturb_interpretable_space` is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b038f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the batch dimension for the embedding-bag model\n",
    "def forward_func(text, offsets):\n",
    "    return eb_model(text.squeeze(0), offsets)\n",
    "\n",
    "# encode text indices into latent representations & calculate cosine similarity\n",
    "def exp_embedding_cosine_distance(original_inp, perturbed_inp, _, **kwargs):\n",
    "    original_emb = eb_model.embedding(original_inp, None)\n",
    "    perturbed_emb = eb_model.embedding(perturbed_inp, None)\n",
    "    distance = 1 - F.cosine_similarity(original_emb, perturbed_emb, dim=1)\n",
    "    return torch.exp(-1 * (distance ** 2) / 2)\n",
    "\n",
    "# binary vector where each word is selected independently and uniformly at random\n",
    "def bernoulli_perturb(text, **kwargs):\n",
    "    probs = torch.ones_like(text) * 0.5\n",
    "    return torch.bernoulli(probs).long()\n",
    "\n",
    "# remove absenst token based on the intepretable representation sample\n",
    "def interp_to_input(interp_sample, original_input, **kwargs):\n",
    "    return original_input[interp_sample.bool()].view(original_input.size(0), -1)\n",
    "\n",
    "lasso_lime_base = LimeBase(\n",
    "    forward_func, \n",
    "    interpretable_model=SkLearnLasso(alpha=0.08),\n",
    "    similarity_func=exp_embedding_cosine_distance,\n",
    "    perturb_func=bernoulli_perturb,\n",
    "    perturb_interpretable_space=True,\n",
    "    from_interp_rep_transform=interp_to_input,\n",
    "    to_interp_rep_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca2cfa",
   "metadata": {},
   "source": [
    "The attribution call is the same as the `Lime` class. Just remember to add the dummy batch dimension to the text input and put the offsets in the `additional_forward_args` because it is not a feature for the classification but a metadata for the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd0ca922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lime Base attribution: 100%|██████████| 32000/32000 [00:22<00:00, 1432.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribution range: -0.4232264757156372 to 0.9536108374595642\n"
     ]
    }
   ],
   "source": [
    "attrs = lasso_lime_base.attribute(\n",
    "    test_text.unsqueeze(0), # add batch dimension for Captum\n",
    "    target=test_labels,\n",
    "    additional_forward_args=(test_offsets,),\n",
    "    n_samples=32000,\n",
    "    show_progress=True\n",
    ").squeeze(0)\n",
    "\n",
    "print('Attribution range:', attrs.min().item(), 'to', attrs.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bd4a8",
   "metadata": {},
   "source": [
    "At last, let us create a simple visualization to highlight the influential words where green stands for positive correlation and red for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "028c7d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><mark style=\"background-color:rgba(255,0,0,0.45640903078960515)\">us</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">men</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">have</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">right</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">touch</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">in</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">relay</mark> <mark style=\"background-color:rgba(255,0,0,0.3798096495636427)\">duel</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">against</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">australia</mark> <mark style=\"background-color:rgba(0,255,0,0.9765299982384382)\">thens</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">,</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">aug</mark> <mark style=\"background-color:rgba(255,0,0,0.5595067172836139)\">.</mark> <mark style=\"background-color:rgba(0,255,0,0.2644781853030491)\">17</mark> <mark style=\"background-color:rgba(255,0,0,0.6505585874582221)\">-</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">so</mark> <mark style=\"background-color:rgba(255,0,0,0.5975870827935951)\">michael</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">phelps</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">is</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">not</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">going</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">to</mark> <mark style=\"background-color:rgba(0,255,0,0.821556492286908)\">match</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">the</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">seven</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">gold</mark> <mark style=\"background-color:rgba(0,255,0,0.4329777371187277)\">medals</mark> <mark style=\"background-color:rgba(0,255,0,0.12680978752758576)\">won</mark> <mark style=\"background-color:rgba(255,0,0,0.3485568949642219)\">by</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">mark</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">spitz</mark> <mark style=\"background-color:rgba(255,0,0,0.5688951988163126)\">.</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">and</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">it</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">is</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">too</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">early</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">to</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">tell</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">if</mark> <mark style=\"background-color:rgba(0,255,0,0.5334755337904876)\">he</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">will</mark> <mark style=\"background-color:rgba(0,255,0,0.8323776168908136)\">match</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">aleksandr</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">dityatin</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">,</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">the</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">soviet</mark> <mark style=\"background-color:rgba(0,255,0,0.6241970149184269)\">gymnast</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">who</mark> <mark style=\"background-color:rgba(0,255,0,0.13470780472922608)\">won</mark> <mark style=\"background-color:rgba(0,255,0,0.3170152619355239)\">eight</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">total</mark> <mark style=\"background-color:rgba(0,255,0,0.4189161099301892)\">medals</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">in</mark> <mark style=\"background-color:rgba(0,255,0,0.0)\">1980</mark> <mark style=\"background-color:rgba(255,0,0,0.5665426944946758)\">.</mark></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_text_attr(attrs):\n",
    "    rgb = lambda x: '255,0,0' if x < 0 else '0,255,0'\n",
    "    alpha = lambda x: abs(x) ** 0.5\n",
    "    token_marks = [\n",
    "        f'<mark style=\"background-color:rgba({rgb(attr)},{alpha(attr)})\">{token}</mark>'\n",
    "        for token, attr in zip(tokenizer(test_line), attrs.tolist())\n",
    "    ]\n",
    "    \n",
    "    display(HTML('<p>' + ' '.join(token_marks) + '</p>'))\n",
    "    \n",
    "show_text_attr(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b3045",
   "metadata": {},
   "source": [
    "The above visulization should render something like the image below where the model links the \"Sports\" subject to many reasonable words, like \"match\" and \"medals\".\n",
    "\n",
    "![Lime Text](img/lime_text_viz.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
